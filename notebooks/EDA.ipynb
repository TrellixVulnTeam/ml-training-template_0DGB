{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNXk2VKr8Jh1"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbERbpJD4eCz"
      },
      "source": [
        "\"Divorce Predictors data set Data Set\" dataset publicly available [here](https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bx4CqIT4tEp"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00497/divorce.rar -P ml/input/data/training/\n",
        "!cd ml/input/data/training/ && unrar e divorce.rar\n",
        "!cd ml/input/data/training/ && rm divorce.rar && rm divorce.xlsx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52QZrHYu8PTm"
      },
      "source": [
        "# EDA (Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CPo1enb4eC0"
      },
      "source": [
        "input_dir = 'ml/input/data/training/divorce.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9eQGH7L4eC5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(input_dir, sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvSS8Jbp4eC9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9--BOU4_4eDA"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTVZF8pj4eDD"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "# calculate the correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# plot the heatmap\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPyls05k4eDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFRnp1508gZN"
      },
      "source": [
        "# PoC / Benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fe_NBenG-0N"
      },
      "source": [
        "## Install some stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pam3nVfwAG2x"
      },
      "source": [
        "!pip install dask[dataframe]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VumuKQGkHDSJ"
      },
      "source": [
        "## Define some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q_St3W2_1_Z"
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "import tarfile\n",
        "from abc import ABC, abstractmethod\n",
        "from io import StringIO\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import yaml\n",
        "from dask.dataframe import from_delayed\n",
        "from dask.delayed import delayed\n",
        "from pandas import read_csv\n",
        "\n",
        "\n",
        "class BaseFile(ABC):\n",
        "\n",
        "    @staticmethod\n",
        "    @abstractmethod\n",
        "    def read(origin, **kwargs):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    @abstractmethod\n",
        "    def write(destination, content):\n",
        "        pass\n",
        "\n",
        "class PickleFile(BaseFile):\n",
        "\n",
        "    @staticmethod\n",
        "    def read(origin: str, **kwargs) -> dict:\n",
        "        print(f\"read data from {origin}\")\n",
        "        with open(origin, 'rb') as file:\n",
        "            documents = pickle.load(file)\n",
        "        return documents\n",
        "\n",
        "    @staticmethod\n",
        "    def write(destination: str, content) -> None:\n",
        "        print(f\"save data to {destination}\")\n",
        "        with open(destination, 'wb') as file:\n",
        "            pickle.dump(content, file)\n",
        "\n",
        "\n",
        "class TarFile:\n",
        "\n",
        "    @staticmethod\n",
        "    def uncompress(origin: str, path: str):\n",
        "        with tarfile.open(origin) as tar:\n",
        "            tar.extractall(path=path)\n",
        "\n",
        "    @staticmethod\n",
        "    def compress(destination: str, content: Dict[str, str]) -> None:\n",
        "        with tarfile.open(destination, \"w:gz\") as tar:\n",
        "            for local_path, tar_path in content.items():\n",
        "                tar.add(local_path, arcname=tar_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hekboc6kHGvJ"
      },
      "source": [
        "## Experiment logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXgTE7a1_Cgz"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "@dataclass\n",
        "class ExperimentArtifacts:\n",
        "    run_tag: str\n",
        "    model_name: str\n",
        "    base_path: Path\n",
        "\n",
        "    def _create_if_not_exist(self):\n",
        "        Path(self.output_prefix).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    @property\n",
        "    def output_prefix(self):\n",
        "        return self.base_path / self.model_name\n",
        "\n",
        "    def generate_artifacts(self, metrics: Dict[str, Any]):\n",
        "        self.metrics = metrics\n",
        "        print(metrics)\n",
        "        metric_names = []\n",
        "        for k in metrics.keys():\n",
        "            metric_names.append(k)\n",
        "\n",
        "    def save_results(self):\n",
        "        self._create_if_not_exist()\n",
        "\n",
        "        metrics_path = str(self.output_prefix / 'metrics.pkl')\n",
        "        PickleFile.write(metrics_path, self.metrics)\n",
        "\n",
        "    def save(self):\n",
        "        self.save_results()\n",
        "\n",
        "    # Create single output for Sagemaker training-job\n",
        "    @property\n",
        "    def model_package_path(self):\n",
        "        return self.base_path / 'model.tar.gz'\n",
        "\n",
        "    def create_package_with_models(self):\n",
        "        print(f\"Loading models from {self.base_path}\")\n",
        "        model_paths = {}\n",
        "        for p in sorted(self.base_path.glob(\"**/*joblib\")):\n",
        "            tar_path = f\"{p.parent.name}/{p.name}\"\n",
        "            model_paths[str(p)] = tar_path\n",
        "        TarFile.compress(self.model_package_path, model_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Actb4sO08kLY"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Experiment:\n",
        "    model: Any\n",
        "    input_dir: Path\n",
        "    artifacts_handler: ExperimentArtifacts\n",
        "    training_portion: float = .8\n",
        "    random_state: int = 42\n",
        "\n",
        "    def load_data(self):\n",
        "        data = np.genfromtxt(self.input_dir, delimiter=';', skip_header=1)\n",
        "        self.X, self.y = data[:, :-1], data[:, -1]\n",
        "\n",
        "    def split_data(self):\n",
        "        self.X_train, self.X_validation, self.y_train, self.y_validation = train_test_split(\n",
        "            self.X,\n",
        "            self.y,\n",
        "            train_size=self.training_portion,\n",
        "            random_state=self.random_state,\n",
        "            stratify=self.y,\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        self.history = self.model.train(\n",
        "            self.X_train,\n",
        "            self.y_train,\n",
        "            self.X_validation,\n",
        "            self.y_validation,\n",
        "        )\n",
        "\n",
        "    def save(self):\n",
        "        self.artifacts_handler.save()\n",
        "        self.model.save(self.artifacts_handler.output_prefix)\n",
        "\n",
        "    def run(self):\n",
        "        self.load_data()\n",
        "        self.split_data()\n",
        "\n",
        "        self.train()\n",
        "\n",
        "        self.artifacts_handler.generate_artifacts(\n",
        "            self.history\n",
        "        )\n",
        "        self.save()\n",
        "        self.artifacts_handler.create_package_with_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP89JBDLHLc5"
      },
      "source": [
        "## Build my model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8zezhko_Mik"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import joblib\n",
        "from sklearn import svm\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "class MLModel(ABC):\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def model_id(self) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def save(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def load(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, data):\n",
        "        pass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ProjectModel(MLModel):\n",
        "    model: BaseEstimator = None\n",
        "\n",
        "    @property\n",
        "    def model_id(self) -> str:\n",
        "        return \"divorce\"\n",
        "\n",
        "    def save(self, model_prefix):\n",
        "        self.model_path = model_prefix / 'model.joblib'\n",
        "        joblib.dump(self.model, self.model_path)\n",
        "        return self.model_path\n",
        "\n",
        "    def load(self, model_prefix):\n",
        "        self.model_path = model_prefix / self.model_id / 'model.joblib'\n",
        "        try:\n",
        "            self.model = joblib.load(self.model_path)\n",
        "        except FileExistsError as e:\n",
        "            print(e)\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        return self.model\n",
        "\n",
        "    def __build_model(self):\n",
        "        anova_filter = SelectKBest(f_regression, k=5)\n",
        "        clf = svm.SVC(kernel='linear')\n",
        "        anova_svm = Pipeline(\n",
        "            [\n",
        "                ('anova', anova_filter),\n",
        "                ('svc', clf)\n",
        "            ]\n",
        "        )\n",
        "        self.model = anova_svm.set_params(\n",
        "            anova__k=10,\n",
        "            svc__C=.1,\n",
        "        )\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_validation,\n",
        "        y_validation,\n",
        "    ):\n",
        "        self.__build_model()\n",
        "\n",
        "        self.model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "        )\n",
        "        score = self.model.score(X_validation, y_validation)\n",
        "        metrics = {\n",
        "            'score': score,\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def predict(self, ids, X):\n",
        "        prediction = self.model.predict(X)\n",
        "        return {i: pred for i, pred in zip(ids, prediction)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfYvEjHPHOyU"
      },
      "source": [
        "## Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9akfVeuAVgW"
      },
      "source": [
        "import datetime\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    arguments = {}\n",
        "    arguments[\"input_dir\"] = Path(\"ml/input/data/training/divorce.csv\")\n",
        "    arguments[\"output_dir\"] = Path(\"ml/output\")\n",
        "    arguments[\"project_name\"] = \"divorce-predictor\"\n",
        "    arguments[\"run_tag\"] = datetime.datetime \\\n",
        "            .fromtimestamp(time.time()) \\\n",
        "            .strftime('%Y-%m-%d-%H%M%S')\n",
        "    print(type(arguments.keys()),arguments.keys())\n",
        "    args = namedtuple('args', list(arguments.keys()))\n",
        "    return args(**arguments)\n",
        "\n",
        "print(f\"Begin train\")\n",
        "\n",
        "args = get_arguments()\n",
        "\n",
        "run_tag = f\"{args.project_name}-{args.run_tag}\"\n",
        "dataset_path = args.input_dir\n",
        "\n",
        "model_name = dataset_path.stem\n",
        "model = ProjectModel()\n",
        "\n",
        "artifacts_handler = ExperimentArtifacts(\n",
        "    run_tag=run_tag,\n",
        "    model_name=model_name,\n",
        "    base_path=args.output_dir,\n",
        ")\n",
        "experiment = Experiment(\n",
        "    model=model,\n",
        "    input_dir=dataset_path,\n",
        "    artifacts_handler=artifacts_handler,\n",
        ")\n",
        "experiment.run()\n",
        "\n",
        "print(f\"End train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9WffRbyAlLA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}